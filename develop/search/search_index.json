{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#runn-random-utility-neural-network","title":"RUNN: Random Utility Neural Network","text":""},{"location":"contributing/","title":"Contributing","text":"<p>runn is an actively maintained and utilised project.</p>"},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":"<p>to report issues, request features, or exchange with our community, just follow the links below.</p> <p>Is something not working?</p> <p> Report a bug</p> <p>Missing information in our docs?</p> <p> Report a docs issue</p> <p>Want to submit an idea?</p> <p> Request a change</p> <p>Have a question or need help?</p> <p> Ask a question</p>"},{"location":"contributing/#developing-runn","title":"Developing runn","text":"<p>To find beginner-friendly existing bugs and feature requests you may like to start out with, take a look at our good first issues.</p>"},{"location":"contributing/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>To create a development environment for runn, with all libraries required for development and quality assurance installed, it is easiest to install runn using the mamba package manager, as follows:</p> <ol> <li>Install mamba with the Mambaforge executable for your operating system.</li> <li>Open the command line (or the \"miniforge prompt\" in Windows).</li> <li>Download (a.k.a., clone) the runn repository: <code>git clone git@github.com:JoseAngelMartinB/runn.git</code></li> <li>Change into the <code>runn</code> directory: <code>cd runn</code></li> <li>Create the runn mamba environment: <code>mamba create -n runn -c conda-forge --file requirements/base.txt --file requirements/dev.txt</code></li> <li>Activate the runn mamba environment: <code>mamba activate runn</code></li> <li>Install the runn package into the environment, in editable mode and ignoring dependencies (we have dealt with those when creating the mamba environment): <code>pip install --no-deps -e .</code></li> </ol> <p>All together:</p> <pre><code>git clone git@github.com:JoseAngelMartinB/runn.git\ncd runn\nmamba create -n runn -c conda-forge --file requirements/base.txt --file requirements/dev.txt\nmamba activate runn\npip install --no-deps -e .\n</code></pre> <p>If installing directly with pip, you can install these libraries using the <code>dev</code> option, i.e., <code>pip install -e '.[dev]'</code> Either way, you should add your environment as a jupyter kernel, so the example notebooks can run in the tests: <code>ipython kernel install --user --name=runn</code> If you plan to make changes to the code then please make regular use of the following tools to verify the codebase while you work:</p> <ul> <li><code>pre-commit</code>: run <code>pre-commit install</code> in your command line to load inbuilt checks that will run every time you commit your changes. The checks are: 1. check no large files have been staged, 2. lint python files for major errors, 3. format python files to conform with the PEP8 standard. You can also run these checks yourself at any time to ensure staged changes are clean by calling <code>pre-commit</code>.</li> <li><code>pytest</code> - run the unit test suite and check test coverage.</li> </ul> <p>Note</p> <p>If you already have an environment called <code>runn</code> on your system (e.g., for a stable installation of the package), you will need to chose a different environment name. You will then need to add this as a pytest argument when running the tests: <code>pytest --nbmake-kernel=[my-env-name]</code>.</p>"},{"location":"contributing/#rapid-fire-testing","title":"Rapid-fire testing","text":"<p>The following options allow you to strip down the test suite to the bare essentials: 1. The test suite includes unit tests and integration tests (in the form of jupyter notebooks found in the <code>examples</code> directory). The integration tests can be slow, so if you want to avoid them during development, you should run <code>pytest tests/</code>. 2. You can avoid generating coverage reports, by adding the <code>--no-cov</code> argument: <code>pytest --no-cov</code>. 3. By default, the tests run with up to two parallel threads, to increase this to e.g. 4 threads: <code>pytest -n4</code>.</p> <p>All together:</p> <pre><code>pytest tests/ --no-cov -n4\n</code></pre> <p>Note</p> <p>You cannot debug failing tests and have your tests run in parallel, you will need to set <code>-n0</code> if using the <code>--pdb</code> flag</p>"},{"location":"contributing/#memory-profiling","title":"Memory profiling","text":"<p>Note</p> <p>When you open a pull request (PR), one of the GitHub actions will run memory profiling for you. This means you don't have to do any profiling locally. However, if you can, it is still good practice to do so as you will catch issues earlier.</p> <p>runn can be memory intensive; we like to ensure that any development to the core code does not exacerbate this. If you are running on a UNIX device (i.e., not on Windows), you can test whether any changes you have made adversely impact memory and time performance as follows:</p> <ol> <li>Install memray in your <code>runn</code> mamba environment: <code>mamba install memray pytest-memray</code>.</li> <li>Run the memory profiling integration test: <code>pytest -p memray -m \"high_mem\" --no-cov</code>.</li> <li>Optionally, to visualise the memory allocation, run <code>pytest -p memray -m \"high_mem\" --no-cov --memray-bin-path=[my_path] --memray-bin-prefix=[my_prefix]</code> - where you must define <code>[my_path]</code> and <code>[my_prefix]</code> - followed by <code>memray flamegraph [my_path]/[my_prefix]-tests-test_100_memory_profiling.py-test_mem.bin</code>. You will then find the HTML report at <code>[my_path]/memray-flamegraph-[my_prefix]-tests-test_100_memory_profiling.py-test_mem.html</code>.</li> </ol> <p>All together:</p> <pre><code>mamba install memray pytest-memray\npytest -p memray -m \"high_mem\" --no-cov --memray-bin-path=[my_path] --memray-bin-prefix=[my_prefix]\nmemray flamegraph [my_path]/[my_prefix]-tests-test_100_memory_profiling.py-test_mem.bin\n</code></pre> <p>For more information on using memray, refer to their documentation.</p>"},{"location":"contributing/#submitting-changes","title":"Submitting changes","text":"<p>To contribute changes:</p> <ol> <li>Fork the project on GitHub.</li> <li>Create a feature branch to work on in your fork (<code>git checkout -b new-fix-or-feature</code>).</li> <li>Test your changes using <code>pytest</code>.</li> <li>Commit your changes to the feature branch (you should have <code>pre-commit</code> installed to ensure your code is correctly formatted when you commit changes).</li> <li>Push the branch to GitHub (<code>git push origin new-fix-or-feature</code>).</li> <li>On GitHub, create a new pull request from the feature branch.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":"<p>Before submitting a pull request, check whether you have:</p> <ul> <li>Added your changes to <code>CHANGELOG.md</code>.</li> <li>Added or updated documentation for your changes.</li> <li>Added tests if you implemented new functionality.</li> </ul> <p>When opening a pull request, please provide a clear summary of your changes!</p>"},{"location":"contributing/#commit-messages","title":"Commit messages","text":"<p>Please try to write clear commit messages. One-line messages are fine for small changes, but bigger changes should look like this:</p> <pre><code>A brief summary of the commit (max 50 characters)\n\nA paragraph or bullet-point list describing what changed and its impact,\ncovering as many lines as needed.\n</code></pre>"},{"location":"contributing/#code-conventions","title":"Code conventions","text":"<p>Start reading our code and you'll get the hang of it.</p> <p>We mostly follow the official Style Guide for Python Code (PEP8).</p> <p>We have chosen to use the uncompromising code formatter <code>black</code> and the linter <code>ruff</code>. When run from the root directory of this repo, <code>pyproject.toml</code> should ensure that formatting and linting fixes are in line with our custom preferences (e.g., 100 character maximum line length). The philosophy behind using <code>black</code> is to have uniform style throughout the project dictated by code. Since <code>black</code> is designed to minimise diffs, and make patches more human readable, this also makes code reviews more efficient. To make this a smooth experience, you should run <code>pre-commit install</code> after setting up your development environment, so that <code>black</code> makes all the necessary fixes to your code each time you commit, and so that <code>ruff</code> will highlight any errors in your code. If you prefer, you can also set up your IDE to run these two tools whenever you save your files, and to have <code>ruff</code> highlight erroneous code directly as you type. Take a look at their documentation for more information on configuring this.</p> <p>We require all new contributions to have docstrings for all modules, classes and methods. When adding docstrings, we request you use the Google docstring style.</p>"},{"location":"contributing/#release-checklist","title":"Release checklist","text":""},{"location":"contributing/#pre-release","title":"Pre-release","text":"<ul> <li> Make sure all unit and integration tests pass (This is best done by creating a pre-release pull request).</li> <li> Re-run tutorial Jupyter notebooks (<code>pytest examples/ --overwrite</code>).</li> <li> Make sure documentation builds without errors (<code>mike deploy [version]</code>, where <code>[version]</code> is the current minor release of the form <code>X.Y</code>).</li> <li> Make sure the changelog is up-to-date, especially that new features and backward incompatible changes are clearly marked.</li> </ul>"},{"location":"contributing/#create-release","title":"Create release","text":"<ul> <li> Bump the version number in <code>runn/__init__.py</code></li> <li> Update the changelog with final version number of the form <code>vX.Y.Z</code>, release date, and github <code>compare</code> link (at the bottom of the page).</li> <li> Commit with message <code>Release vX.Y.Z</code>, then add a <code>vX.Y.Z</code> tag.</li> <li> Create a release pull request to verify that the conda package builds successfully.</li> <li> Once the PR is approved and merged, create a release through the GitHub web interface, using the same tag, titling it <code>Release vX.Y.Z</code> and include all the changelog elements that are not flagged as internal.</li> </ul>"},{"location":"contributing/#post-release","title":"Post-release","text":"<ul> <li> Update the changelog, adding a new <code>[Unreleased]</code> heading.</li> <li> Update <code>runn/__init__.py</code> to the next version appended with <code>.dev0</code>, in preparation for the next main commit.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#setting-up-a-user-environment","title":"Setting up a user environment","text":"<p>As a <code>runn</code> user, it is easiest to install using the mamba package manager, as follows:</p> <ol> <li>Install mamba with the Mambaforge executable for your operating system.</li> <li> <p>Open the command line (or the \"miniforge prompt\" in Windows).</p> </li> <li> <p>Create the runn mamba environment: <code>mamba create -n runn -c conda-forge -c JoseAngelMartinB runn</code></p> </li> <li>Activate the runn mamba environment: <code>mamba activate runn</code></li> </ol> <p>All together:</p> <pre><code>mamba create -n runn -c conda-forge -c JoseAngelMartinB runn\n</code></pre>"},{"location":"installation/#running-the-example-notebooks","title":"Running the example notebooks","text":"<p>If you have followed the non-developer installation instructions above, you will need to install <code>jupyter</code> into your <code>runn</code> environment to run the example notebooks:</p> <pre><code>mamba install -n runn jupyter\n</code></pre> <p>With Jupyter installed, it's easiest to then add the environment as a jupyter kernel:</p> <pre><code>mamba activate runn\nipython kernel install --user --name=runn\njupyter notebook\n</code></pre>"},{"location":"installation/#choosing-a-different-environment-name","title":"Choosing a different environment name","text":"<p>If you would like to use a different name to <code>runn</code> for your mamba environment, the installation becomes (where <code>[my-env-name]</code> is your preferred name for the environment):</p> <pre><code>mamba create -n [my-env-name] -c conda-forge --file requirements/base.txt\nmamba activate [my-env-name]\nipython kernel install --user --name=[my-env-name]\n</code></pre>"},{"location":"installation/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>The install instructions are slightly different to create a development environment compared to a user environment:</p> <pre><code>git clone git@github.com:JoseAngelMartinB/runn.git\ncd runn\nmamba create -n runn -c conda-forge --file requirements/base.txt --file requirements/dev.txt\nmamba activate runn\npip install --no-deps -e .\n</code></pre> <p>For more detailed installation instructions specific to developing the runn codebase, see our development documentation.</p>"},{"location":"examples/1_intro_to_runn/","title":"Introduction to RUNN: Random Utility Neural Network","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport runn\n\nrunn.display_info()\n</pre> %load_ext autoreload %autoreload 2  import runn  runn.display_info()  <pre>2023-11-28 16:00:31.930578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> <pre>\n---------------------------------- RUNN info -----------------------------------\nRUNN: Random Utility Neural Network\nVersion: 0.1.0\nAuthor: Jos\u00e9 \u00c1ngel Mart\u00edn Baos\n\nSystem information:\nTensorFlow version: 2.14.0\nNumber of CPUs available: 1\nNumber of GPUs available: 0\n--------------------------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/1_intro_to_runn/#introduction-to-runn-random-utility-neural-network","title":"Introduction to RUNN: Random Utility Neural Network\u00b6","text":""},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#fixed","title":"Fixed","text":""},{"location":"CHANGELOG/#added","title":"Added","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":""},{"location":"CHANGELOG/#removed","title":"Removed","text":""},{"location":"CHANGELOG/#v010-2023-11-08","title":"[v0.1.0] - 2023-11-08","text":"<p>Initial release.</p>"},{"location":"reference/runn/econometric_indicators/","title":"runn.econometric_indicators","text":"<p>Useful econometric indicators that can be extracted from the models.</p>"},{"location":"reference/runn/econometric_indicators/#runn.econometric_indicators.value_of_time","title":"<code>value_of_time(model, x, time_attribute, cost_attribute, alt, scaler=None)</code>","text":"<p>Calculate the value of time (VOT) for a given alternative. The VOT is calculated for all the observations in the input data.</p> PARAMETER  DESCRIPTION <code>model</code> <p>The model to be used. It should be a model defined in the runn.models module.</p> <p> TYPE: <code>BaseModel</code> </p> <code>x</code> <p>The input data. It can be a tf.Tensor, np.ndarray or pd.DataFrame.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>time_attribute</code> <p>The index or name of the time attribute.</p> <p> TYPE: <code>Union[int, str]</code> </p> <code>cost_attribute</code> <p>The index or name of the cost attribute.</p> <p> TYPE: <code>Union[int, str]</code> </p> <code>alt</code> <p>The index of the alternative to be analysed.</p> <p> TYPE: <code>int</code> </p> <code>scaler</code> <p>If the data was scaled before training the model, the scaler object should be provided. Currently, only the StandardScaler and MinMaxScaler from sklearn.preprocessing are supported. Default: None.</p> <p> TYPE: <code>Optional[object]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Numpy array with the VOT for each observation in the input data.</p> Source code in <code>runn/econometric_indicators.py</code> <pre><code>def value_of_time(\n    model: BaseModel,\n    x: Union[tf.Tensor, np.ndarray, pd.DataFrame],\n    time_attribute: Union[int, str],\n    cost_attribute: Union[int, str],\n    alt: int,\n    scaler: Optional[object] = None,\n) -&gt; np.ndarray:\n    \"\"\"Calculate the value of time (VOT) for a given alternative. The VOT is calculated for all\n    the observations in the input data.\n\n    Args:\n        model: The model to be used. It should be a model defined in the runn.models module.\n        x: The input data. It can be a tf.Tensor, np.ndarray or pd.DataFrame.\n        time_attribute: The index or name of the time attribute.\n        cost_attribute: The index or name of the cost attribute.\n        alt: The index of the alternative to be analysed.\n        scaler: If the data was scaled before training the model, the scaler object should be provided. Currently,\n            only the StandardScaler and MinMaxScaler from sklearn.preprocessing are supported. Default: None.\n\n    Returns:\n        Numpy array with the VOT for each observation in the input data.\n    \"\"\"\n    return -willingness_to_pay(model, x, time_attribute, cost_attribute, alt, scaler)\n</code></pre>"},{"location":"reference/runn/econometric_indicators/#runn.econometric_indicators.willingness_to_pay","title":"<code>willingness_to_pay(model, x, analysed_attribute, cost_attribute, alt, scaler=None)</code>","text":"<p>Calculate the willingness to pay (WTP) for a given attribute and alternative. The WTP is calculated for all the observations in the input data.</p> PARAMETER  DESCRIPTION <code>model</code> <p>The model to be used. It should be a model defined in the runn.models module.</p> <p> TYPE: <code>BaseModel</code> </p> <code>x</code> <p>The input data. It can be a tf.Tensor, np.ndarray or pd.DataFrame.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>analysed_attribute</code> <p>The index or name of the attribute to be analysed.</p> <p> TYPE: <code>Union[int, str]</code> </p> <code>cost_attribute</code> <p>The index or name of the cost attribute.</p> <p> TYPE: <code>Union[int, str]</code> </p> <code>alt</code> <p>The index of the alternative to be analysed.</p> <p> TYPE: <code>int</code> </p> <code>scaler</code> <p>If the data was scaled before training the model, the scaler object should be provided. Currently, only the StandardScaler and MinMaxScaler from sklearn.preprocessing are supported. Default: None.</p> <p> TYPE: <code>Optional[object]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Numpy array with the WTP for each observation in the input data.</p> Source code in <code>runn/econometric_indicators.py</code> <pre><code>def willingness_to_pay(\n    model: BaseModel,\n    x: Union[tf.Tensor, np.ndarray, pd.DataFrame],\n    analysed_attribute: Union[int, str],\n    cost_attribute: Union[int, str],\n    alt: int,\n    scaler: Optional[object] = None,\n) -&gt; np.ndarray:\n    \"\"\"Calculate the willingness to pay (WTP) for a given attribute and alternative. The WTP is calculated for all\n    the observations in the input data.\n\n    Args:\n        model: The model to be used. It should be a model defined in the runn.models module.\n        x: The input data. It can be a tf.Tensor, np.ndarray or pd.DataFrame.\n        analysed_attribute: The index or name of the attribute to be analysed.\n        cost_attribute: The index or name of the cost attribute.\n        alt: The index of the alternative to be analysed.\n        scaler: If the data was scaled before training the model, the scaler object should be provided. Currently,\n            only the StandardScaler and MinMaxScaler from sklearn.preprocessing are supported. Default: None.\n\n    Returns:\n        Numpy array with the WTP for each observation in the input data.\n    \"\"\"\n    if scaler is not None and not isinstance(scaler, (StandardScaler, MinMaxScaler)):\n        raise ValueError(\n            \"The scaler object should be either a StandardScaler or a MinMaxScaler from \" \"sklearn.preprocessing.\"\n        )\n\n    if isinstance(analysed_attribute, str) and isinstance(x, pd.DataFrame):\n        if analysed_attribute not in model.params[\"attributes\"]:\n            raise ValueError(\"The analysed attribute is not present in the model.\")\n        analysed_attribute = x.columns.get_loc(analysed_attribute)\n    elif not isinstance(analysed_attribute, int):\n        raise ValueError(\n            \"The analysed attribute should be either an integer indicating the index of the attribute \"\n            \"or a string with the name of the attribute.\"\n        )\n\n    if isinstance(cost_attribute, str) and isinstance(x, pd.DataFrame):\n        if cost_attribute not in model.params[\"attributes\"]:\n            raise ValueError(\"The cost attribute is not present in the model.\")\n        cost_attribute = x.columns.get_loc(cost_attribute)\n    elif not isinstance(cost_attribute, int):\n        raise ValueError(\n            \"The cost attribute should be either an integer indicating the index of the attribute \"\n            \"or a string with the name of the attribute.\"\n        )\n\n    if analysed_attribute == cost_attribute:\n        raise ValueError(\"The analysed attribute cannot be the same as the cost attribute.\")\n    if analysed_attribute &gt;= len(model.params[\"attributes\"]):\n        raise ValueError(\"The analysed attribute index is out of range.\")\n    if cost_attribute &gt;= len(model.params[\"attributes\"]):\n        raise ValueError(\"The cost attribute index is out of range.\")\n\n    if isinstance(x, pd.DataFrame):\n        x = x.values\n    if isinstance(x, np.ndarray):\n        x = tf.convert_to_tensor(x)\n\n    if alt &gt;= model.params[\"n_alt\"]:\n        raise ValueError(\"The alternative index is out of range.\")\n    if alt &lt; 0:\n        raise ValueError(\"The alternative index cannot be negative.\")\n\n    # Compute the gradient of the utility function with respect to the analysed attributes using the tensorflow\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        pred_utility = model.get_utility(x)\n        pred_utility = pred_utility[:, alt]\n    grad = tape.gradient(pred_utility, x)\n\n    grad_cost = grad[:, cost_attribute]\n    grad_analysed_attr = grad[:, analysed_attribute]\n\n    # Undo the scaling effect on the WTP\n    if scaler is not None:\n        if type(scaler) is StandardScaler:\n            if isinstance(analysed_attribute, str):\n                analysed_attr_scale = scaler.scale_[list(scaler.feature_names_in_).index(analysed_attribute)]\n            else:\n                analysed_attr_scale = scaler.scale_[\n                    list(scaler.feature_names_in_).index(model.params[\"attributes\"][analysed_attribute])\n                ]\n            if isinstance(cost_attribute, str):\n                cost_attr_scale = scaler.scale_[list(scaler.feature_names_in_).index(cost_attribute)]\n            else:\n                cost_attr_scale = scaler.scale_[\n                    list(scaler.feature_names_in_).index(model.params[\"attributes\"][cost_attribute])\n                ]\n            grad_analysed_attr = grad_analysed_attr / analysed_attr_scale\n            grad_cost = grad_cost / cost_attr_scale\n        elif type(scaler) is MinMaxScaler:\n            raise NotImplementedError(\"MinMaxScaler not implemented yet.\")  # TODO: Implement\n\n    # Compute the WTP\n    wtp = -grad_analysed_attr / grad_cost\n    wtp = wtp.numpy()\n\n    return wtp\n</code></pre>"},{"location":"reference/runn/metrics/","title":"runn.metrics","text":"<p>Useful metrics for evaluating the performance of the neural network models.</p>"},{"location":"reference/runn/metrics/#runn.metrics.AMPCA","title":"<code>AMPCA(proba, y)</code>","text":"<p>Arithmetic Mean Probability of Correct Assignment (AMPCA) metric.</p> PARAMETER  DESCRIPTION <code>proba</code> <p>Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array of true choices. Each element corresponds to the index of the chosen alternative.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>AMPCA metric.</p> Source code in <code>runn/metrics.py</code> <pre><code>def AMPCA(proba: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Arithmetic Mean Probability of Correct Assignment (AMPCA) metric.\n\n    Args:\n        proba: Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an\n        alternative.\n        y: Array of true choices. Each element corresponds to the index of the chosen alternative.\n\n    Returns:\n        AMPCA metric.\n    \"\"\"\n    sum = 0\n    i = 0\n    for sel_mode in y:\n        sum = sum + proba[i, sel_mode]\n        i += 1\n    N = i - 1\n    return sum / N\n</code></pre>"},{"location":"reference/runn/metrics/#runn.metrics.CEL","title":"<code>CEL(proba, y)</code>","text":"<p>Cross-Entropy Loss (CEL) metric.</p> PARAMETER  DESCRIPTION <code>proba</code> <p>Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array of true choices. Each element corresponds to the index of the chosen alternative.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>CEL metric.</p> Source code in <code>runn/metrics.py</code> <pre><code>def CEL(proba: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Cross-Entropy Loss (CEL) metric.\n\n    Args:\n        proba: Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an\n        alternative.\n        y: Array of true choices. Each element corresponds to the index of the chosen alternative.\n\n    Returns:\n        CEL metric.\n    \"\"\"\n    sum = 0\n    i = 0\n    for sel_mode in y:\n        sum = sum + np.log(proba[i, sel_mode])\n        i += 1\n    N = i - 1\n    return -sum / N\n</code></pre>"},{"location":"reference/runn/metrics/#runn.metrics.GMPCA","title":"<code>GMPCA(proba, y)</code>","text":"<p>Geometric Mean Probability of Correct Assignment (GMPCA) metric.</p> PARAMETER  DESCRIPTION <code>proba</code> <p>Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array of true choices. Each element corresponds to the index of the chosen alternative.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>GMPCA metric.</p> Source code in <code>runn/metrics.py</code> <pre><code>def GMPCA(proba: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Geometric Mean Probability of Correct Assignment (GMPCA) metric.\n\n    Args:\n        proba: Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an\n        alternative.\n        y: Array of true choices. Each element corresponds to the index of the chosen alternative.\n\n    Returns:\n        GMPCA metric.\n    \"\"\"\n    return np.exp(-CEL(proba, y))\n</code></pre>"},{"location":"reference/runn/metrics/#runn.metrics.accuracy","title":"<code>accuracy(proba, y)</code>","text":"<p>Accuracy metric.</p> PARAMETER  DESCRIPTION <code>proba</code> <p>Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array of true choices. Each element corresponds to the index of the chosen alternative.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Accuracy metric.</p> Source code in <code>runn/metrics.py</code> <pre><code>def accuracy(proba: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Accuracy metric.\n\n    Args:\n        proba: Matrix of predicted choice probabilities. Each row corresponds to a sample and each column to an\n        alternative.\n        y: Array of true choices. Each element corresponds to the index of the chosen alternative.\n\n    Returns:\n        Accuracy metric.\n    \"\"\"\n    return np.mean(np.argmax(proba, axis=1) == y)\n</code></pre>"},{"location":"reference/runn/models/altspec_mono_nn/","title":"runn.models.altspec_mono_nn","text":""},{"location":"reference/runn/models/altspec_mono_nn/#runn.models.altspec_mono_nn.AltSpecMonoNN","title":"<code>AltSpecMonoNN(params=None)</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Alternative-specific monotonic neural network model for choice modeling.</p> PARAMETER  DESCRIPTION <code>params</code> <p>Dictionary with the model parameters.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Source code in <code>runn/models/altspec_mono_nn.py</code> <pre><code>def __init__(self, params: dict = None) -&gt; None:\n    \"\"\"Alternative-specific monotonic neural network model for choice modeling.\n\n    Args:\n        params: Dictionary with the model parameters.\n    \"\"\"\n\n    super().__init__(params)\n</code></pre>"},{"location":"reference/runn/models/base/","title":"runn.models.base","text":""},{"location":"reference/runn/models/base/#runn.models.base.optimizers","title":"<code>optimizers = {'adadelta': Adadelta, 'adafactor': Adafactor, 'adagrad': Adagrad, 'adam': Adam, 'adamw': AdamW, 'adamax': Adamax, 'ftrl': Ftrl, 'lion': Lion, 'nadam': Nadam, 'rmsprop': RMSprop, 'sgd': SGD}</code>  <code>module-attribute</code>","text":""},{"location":"reference/runn/models/base/#runn.models.base.BaseModel","title":"<code>BaseModel(params=None, filename=None)</code>","text":"<p>Abstract base class for all choice models.</p> Source code in <code>runn/models/base.py</code> <pre><code>def __init__(self, params: dict = None, filename: str = None) -&gt; None:\n    self.keras_model = None\n    self.fitted = False\n    self.history = None\n    self.attributes = []\n    if filename is None:\n        # Initialize new model\n        self.params = params\n        self._initilize_base_params()\n    elif isinstance(filename, str):\n        # Load model from file\n        self.load(filename)\n    else:\n        raise ValueError(\"The 'filename' parameter should be a string.\")\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.attributes","title":"<code>attributes = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.fitted","title":"<code>fitted = False</code>  <code>instance-attribute</code>","text":""},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.history","title":"<code>history = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.keras_model","title":"<code>keras_model = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.evaluate","title":"<code>evaluate(x, y, **kwargs)</code>","text":"<p>Returns the loss value &amp; metrics values for the model for a given input.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input data.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>y</code> <p>Target data.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>**kwargs</code> <p>Additional arguments passed to the keras model. See tf.keras.Model.evaluate() for details.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Union[float, list]</code> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has</p> <code>Union[float, list]</code> <p>multiple outputs and/or metrics). See tf.keras.Model.evaluate() for details.</p> Source code in <code>runn/models/base.py</code> <pre><code>def evaluate(\n    self, x: Union[tf.Tensor, np.ndarray, pd.DataFrame], y: Union[tf.Tensor, np.ndarray, pd.DataFrame], **kwargs\n) -&gt; Union[float, list]:\n    \"\"\"Returns the loss value &amp; metrics values for the model for a given input.\n\n    Args:\n        x: Input data.\n        y: Target data.\n        **kwargs: Additional arguments passed to the keras model. See tf.keras.Model.evaluate() for details.\n\n    Returns:\n        Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has\n        multiple outputs and/or metrics). See tf.keras.Model.evaluate() for details.\n    \"\"\"\n    if self.fitted is False:\n        raise Exception(\"The model is not fitted yet. Please call fit() first.\")\n    return self.keras_model.evaluate(x, y, **kwargs)\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.fit","title":"<code>fit(x, y, batch_size=None, epochs=1, verbose=1, callbacks=None, **kwargs)</code>","text":"<p>Train the model for a fixed number of epochs (iterations on a dataset).</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input data.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>y</code> <p>Target data.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>batch_size</code> <p>Number of samples per gradient update. If unspecified, batch_size will default to 32.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>epochs</code> <p>Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>verbose</code> <p>Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>callbacks</code> <p>List of tf.keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks for details. Default: None.</p> <p> TYPE: <code>Optional[list]</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments passed to the keras model. See tf.keras.Model.fit() for details.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A tf.keras.callbacks.History object. Its History.history attribute is a record of training loss values</p> <code>History</code> <p>and metrics values at successive epochs, as well as validation loss values and validation metrics values</p> <code>History</code> <p>(if applicable).</p> Source code in <code>runn/models/base.py</code> <pre><code>def fit(\n    self,\n    x: Union[tf.Tensor, np.ndarray, pd.DataFrame],\n    y: Union[tf.Tensor, np.ndarray, pd.DataFrame],\n    batch_size: Optional[int] = None,\n    epochs: int = 1,\n    verbose: int = 1,\n    callbacks: Optional[list] = None,\n    **kwargs,\n) -&gt; tf.keras.callbacks.History:\n    \"\"\"Train the model for a fixed number of epochs (iterations on a dataset).\n\n    Args:\n        x: Input data.\n        y: Target data.\n        batch_size: Number of samples per gradient update. If unspecified, batch_size will default to 32.\n        epochs: Number of epochs to train the model. An epoch is an iteration over the entire x and y data\n            provided. Default: 1.\n        verbose: Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Default: 1.\n        callbacks: List of tf.keras.callbacks.Callback instances. List of callbacks to apply during training.\n            See tf.keras.callbacks for details. Default: None.\n        **kwargs: Additional arguments passed to the keras model. See tf.keras.Model.fit() for details.\n\n    Returns:\n        A tf.keras.callbacks.History object. Its History.history attribute is a record of training loss values\n        and metrics values at successive epochs, as well as validation loss values and validation metrics values\n        (if applicable).\n    \"\"\"\n    self.history = self.keras_model.fit(x, y, batch_size, epochs, verbose, callbacks, **kwargs)\n    self.fitted = True\n    return self.history\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.load","title":"<code>load()</code>  <code>abstractmethod</code>","text":"Source code in <code>runn/models/base.py</code> <pre><code>@abstractmethod\ndef load(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.plot_model","title":"<code>plot_model(filename=None, expand_nested=True, dpi=96)</code>","text":"<p>Generate a graphical representation of the model.</p> PARAMETER  DESCRIPTION <code>filename</code> <p>File to which the plot will be saved. If None, the plot will only be displayed on screen. Default: None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>expand_nested</code> <p>Whether to expand nested models into clusters. Default: True.</p> <p> DEFAULT: <code>True</code> </p> <code>dpi</code> <p>Resolution of the plot. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> Source code in <code>runn/models/base.py</code> <pre><code>def plot_model(self, filename: str = None, expand_nested=True, dpi: int = 96) -&gt; None:\n    \"\"\"Generate a graphical representation of the model.\n\n    Args:\n        filename: File to which the plot will be saved. If None, the plot will only be displayed on screen. Default:\n            None.\n        expand_nested: Whether to expand nested models into clusters. Default: True.\n        dpi: Resolution of the plot. Default: 96.\n    \"\"\"\n    if self.keras_model is None:\n        raise ValueError(\"Keras model is not initialized yet. Please call build() first.\")\n    if filename is None:\n        filename = self.__class__.__name__ + \".png\"\n    return plot_model(\n        self.keras_model,\n        show_shapes=True,\n        show_layer_names=True,\n        expand_nested=expand_nested,\n        rankdir=\"TB\",\n        style=0,\n        color=True,\n        to_file=filename,\n        dpi=dpi,\n    )\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.predict","title":"<code>predict(x, **kwargs)</code>","text":"<p>Predict the choice probabilities for a given input.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input data.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> <code>**kwargs</code> <p>Additional arguments passed to the keras model. See tf.keras.Model.predict() for details.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor with the predicted choice probabilities.</p> Source code in <code>runn/models/base.py</code> <pre><code>def predict(self, x: Union[tf.Tensor, np.ndarray, pd.DataFrame], **kwargs) -&gt; tf.Tensor:\n    \"\"\"Predict the choice probabilities for a given input.\n\n    Args:\n        x: Input data.\n        **kwargs: Additional arguments passed to the keras model. See tf.keras.Model.predict() for details.\n\n    Returns:\n        Tensor with the predicted choice probabilities.\n    \"\"\"\n    if self.fitted is False:\n        raise Exception(\"The model is not fitted yet. Please call fit() first.\")\n    return self.keras_model.predict(x, **kwargs)\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.save","title":"<code>save()</code>  <code>abstractmethod</code>","text":"Source code in <code>runn/models/base.py</code> <pre><code>@abstractmethod\ndef save(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/runn/models/base/#runn.models.base.BaseModel.summary","title":"<code>summary()</code>","text":"<p>Print a summary of the keras model.</p> Source code in <code>runn/models/base.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Print a summary of the keras model.\"\"\"\n    if self.keras_model is None:\n        raise Exception(\"Keras model is not initialized yet. Please call build() first.\")\n    self.keras_model.summary()\n</code></pre>"},{"location":"reference/runn/models/dnn/","title":"runn.models.dnn","text":""},{"location":"reference/runn/models/dnn/#runn.models.dnn.DNN","title":"<code>DNN(params=None, filename=None)</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Deep neural network model for choice modeling.</p> PARAMETER  DESCRIPTION <code>params</code> <p>Dictionary with the model parameters. Default: None.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>filename</code> <p>Load a previously trained model from a file. Default: None.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>runn/models/dnn.py</code> <pre><code>def __init__(self, params: dict = None, filename: str = None) -&gt; None:\n    \"\"\"Deep neural network model for choice modeling.\n\n    Args:\n        params: Dictionary with the model parameters. Default: None.\n        filename: Load a previously trained model from a file. Default: None.\n    \"\"\"\n    super().__init__(params, filename)\n    if filename is None:\n        self._initilize_dnn_params()\n        self._build()\n    self._compile()\n</code></pre>"},{"location":"reference/runn/models/dnn/#runn.models.dnn.DNN.get_utility","title":"<code>get_utility(x)</code>","text":"<p>Get the utility of each alternative for a given set of observations.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The input data. It can be a tf.Tensor, np.ndarray or pd.DataFrame.</p> <p> TYPE: <code>Union[Tensor, ndarray, DataFrame]</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Numpy array with the utility of each alternative for each observation in the input data.</p> Source code in <code>runn/models/dnn.py</code> <pre><code>def get_utility(self, x: Union[tf.Tensor, np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"Get the utility of each alternative for a given set of observations.\n\n    Args:\n        x: The input data. It can be a tf.Tensor, np.ndarray or pd.DataFrame.\n\n    Returns:\n        Numpy array with the utility of each alternative for each observation in the input data.\n    \"\"\"\n    if self.fitted is False:\n        raise Exception(\"The model is not fitted yet. Please call fit() first.\")\n\n    if isinstance(x, pd.DataFrame):\n        x = x.values\n    if isinstance(x, np.ndarray):\n        x = tf.convert_to_tensor(x)\n\n    utility_model = Model(inputs=self.keras_model.input, outputs=self.keras_model.get_layer(\"U\").output)\n    return utility_model(x)\n</code></pre>"},{"location":"reference/runn/models/dnn/#runn.models.dnn.DNN.load","title":"<code>load(path)</code>","text":"<p>Load the model from a file.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the file where the model is saved.</p> <p> TYPE: <code>str</code> </p> Source code in <code>runn/models/dnn.py</code> <pre><code>def load(self, path: str) -&gt; None:\n    \"\"\"Load the model from a file.\n\n    Args:\n        path: Path to the file where the model is saved.\n    \"\"\"\n    if not isinstance(path, str):\n        raise ValueError(\"The 'path' parameter should be a string.\")\n    # Check that the str ends with .zip\n    if not path.endswith(\".zip\"):\n        raise ValueError(\"The 'path' parameter should be a .zip file.\")\n    else:\n        # Remove the .zip extension\n        aux_files = path[:-4]\n        # Get the last index of the '/' character\n        idx = aux_files.rfind(\"/\")\n        # Get the name of the file without the path\n        aux_name = aux_files[idx + 1 :]\n\n    try:\n        # Extract the files inside an temporal auxiliary folder\n        os.mkdir(aux_files)\n        with ZipFile(path, \"r\") as zip:\n            zip.extractall(path=aux_files)\n\n        # Load model info\n        with open(aux_files + \"/\" + aux_name + \"_info.json\", \"r\") as f:\n            model_info = json.load(f)\n        if model_info[\"model\"] != \"DNN\":\n            raise ValueError(\"The model in the file is not a DNN model.\")\n\n        # Check runn version\n        major, minor, patch = model_info[\"runn_version\"].split(\".\")\n        if (\n            int(major) &gt; int(runn.__version__.split(\".\")[0])\n            or (\n                int(major) == int(runn.__version__.split(\".\")[0])\n                and int(minor) &gt; int(runn.__version__.split(\".\")[1])\n            )\n            or (\n                int(major) == int(runn.__version__.split(\".\")[0])\n                and int(minor) == int(runn.__version__.split(\".\")[1])\n                and int(patch) &gt; int(runn.__version__.split(\".\")[2])\n            )\n        ):\n            msg = (\n                \"The model was created with a newer version of runn ({}). \"\n                \"Please update runn to version {} or higher.\".format(model_info[\"runn_version\"], runn.__version__)\n            )\n            warnings.warn(msg)\n\n        # Load the parameters\n        self.params = pickle.load(open(aux_files + \"/\" + aux_name + \"_params.pkl\", \"rb\"))\n\n        # Load the keras model\n        self._build()\n        self.keras_model.load_weights(aux_files + \"/\" + aux_name + \"_model.h5\")\n\n        # Load the history\n        self.history = pickle.load(open(aux_files + \"/\" + aux_name + \"_history.pkl\", \"rb\"))\n        self.fitted = model_info[\"fitted\"]\n    except Exception as e:\n        raise e\n    finally:\n        # Delete the auxiliary folder\n        for file in os.listdir(aux_files):\n            os.remove(aux_files + \"/\" + file)\n        os.rmdir(aux_files)\n</code></pre>"},{"location":"reference/runn/models/dnn/#runn.models.dnn.DNN.save","title":"<code>save(path='model.zip')</code>","text":"<p>Save the model to a file.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the file where the model will be saved. Default: 'model.zip'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'model.zip'</code> </p> Source code in <code>runn/models/dnn.py</code> <pre><code>def save(self, path: str = \"model.zip\") -&gt; None:\n    \"\"\"Save the model to a file.\n\n    Args:\n        path: Path to the file where the model will be saved. Default: 'model.zip'.\n    \"\"\"\n    if not isinstance(path, str):\n        raise ValueError(\"The 'path' parameter should be a string.\")\n    if path[-3:] != \".zip\":\n        path += \".zip\"\n    aux_files = path[:-4]\n\n    files = []\n    # Save model info as json\n    model_info = {\n        \"model\": \"DNN\",\n        \"runn_version\": runn.__version__,\n        \"creation_date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"fitted\": self.fitted,\n    }\n    with open(aux_files + \"_info.json\", \"w\") as f:\n        json.dump(model_info, f)\n    files.append(aux_files + \"_info.json\")\n\n    # Save the parameters\n    pickle.dump(self.params, open(aux_files + \"_params.pkl\", \"wb\"))\n    files.append(aux_files + \"_params.pkl\")\n\n    # Save the keras model\n    self.keras_model.save_weights(aux_files + \"_model.h5\")\n    files.append(aux_files + \"_model.h5\")\n\n    # Save the history\n    pickle.dump(self.history, open(aux_files + \"_history.pkl\", \"wb\"))\n    files.append(aux_files + \"_history.pkl\")\n\n    # Compress all the files\n    with ZipFile(path, \"w\") as zip:\n        for file in files:\n            zip.write(file, os.path.basename(file))\n\n    # Delete the auxiliary files\n    for file in files:\n        os.remove(file)\n</code></pre>"},{"location":"reference/runn/models/runn/","title":"runn.models.runn","text":""},{"location":"reference/runn/models/runn/#runn.models.runn.RUNN","title":"<code>RUNN(params=None, filename=None)</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>runn/models/base.py</code> <pre><code>def __init__(self, params: dict = None, filename: str = None) -&gt; None:\n    self.keras_model = None\n    self.fitted = False\n    self.history = None\n    self.attributes = []\n    if filename is None:\n        # Initialize new model\n        self.params = params\n        self._initilize_base_params()\n    elif isinstance(filename, str):\n        # Load model from file\n        self.load(filename)\n    else:\n        raise ValueError(\"The 'filename' parameter should be a string.\")\n</code></pre>"},{"location":"reference/runn/plot_model/","title":"runn.plot_model","text":"<p>Utilities related to model visualization.</p> <p>This file is based on the implementation https://github.com/dario-passos/plot_model/tree/master which is a for of https://github.com/Qinbf/plot_model.git distributed under MIT license.</p>"},{"location":"reference/runn/plot_model/#runn.plot_model.add_edge","title":"<code>add_edge(dot, src, dst, output_shape=None)</code>","text":"Source code in <code>runn/plot_model.py</code> <pre><code>def add_edge(dot, src, dst, output_shape=None):\n    if not dot.get_edge(src, dst):\n        if output_shape:\n            dot.add_edge(pydot.Edge(src, dst, label=output_shape))\n        else:\n            dot.add_edge(pydot.Edge(src, dst))\n</code></pre>"},{"location":"reference/runn/plot_model/#runn.plot_model.check_pydot","title":"<code>check_pydot()</code>","text":"<p>Returns True if PyDot and Graphviz are available.</p> Source code in <code>runn/plot_model.py</code> <pre><code>def check_pydot():\n    \"\"\"Returns True if PyDot and Graphviz are available.\"\"\"\n    if pydot is None:\n        return False\n    try:\n        # Attempt to create an image of a blank graph\n        # to check the pydot/graphviz installation.\n        pydot.Dot.create(pydot.Dot())\n        return True\n    except OSError:\n        return False\n</code></pre>"},{"location":"reference/runn/plot_model/#runn.plot_model.is_wrapped_model","title":"<code>is_wrapped_model(layer)</code>","text":"Source code in <code>runn/plot_model.py</code> <pre><code>def is_wrapped_model(layer):\n    from tensorflow.keras import Model as Network\n    from tensorflow.keras.layers import Wrapper\n\n    return isinstance(layer, Wrapper) and isinstance(layer.layer, Network)\n</code></pre>"},{"location":"reference/runn/plot_model/#runn.plot_model.model_to_dot","title":"<code>model_to_dot(model, show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96, style=0, color=True, subgraph=False)</code>","text":"<p>Convert a Keras model to dot format.</p> PARAMETER  DESCRIPTION <code>model</code> <p>A Keras model instance.</p> <p> </p> <code>show_shapes</code> <p>whether to display shape information.</p> <p> DEFAULT: <code>False</code> </p> <code>show_layer_names</code> <p>whether to display layer names.</p> <p> DEFAULT: <code>True</code> </p> <code>rankdir</code> <p><code>rankdir</code> argument passed to PyDot,   a string specifying the format of the plot:   'TB' creates a vertical plot;   'LR' creates a horizontal plot.</p> <p> DEFAULT: <code>'TB'</code> </p> <code>expand_nested</code> <p>whether to expand nested models into clusters.</p> <p> DEFAULT: <code>False</code> </p> <code>dpi</code> <p>Dots per inch.</p> <p> DEFAULT: <code>96</code> </p> <code>style</code> <p>value 0,1.</p> <p> DEFAULT: <code>0</code> </p> <code>color</code> <p>whether to display color.</p> <p> DEFAULT: <code>True</code> </p> <code>subgraph</code> <p>whether to return a <code>pydot.Cluster</code> instance.</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>A <code>pydot.Dot</code> instance representing the Keras model or</p> <p>a <code>pydot.Cluster</code> instance representing nested model if</p> <p><code>subgraph=True</code>.</p> RAISES DESCRIPTION <code>ImportError</code> <p>if graphviz or pydot are not available.</p> Source code in <code>runn/plot_model.py</code> <pre><code>def model_to_dot(\n    model,\n    show_shapes=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n    style=0,\n    color=True,\n    subgraph=False,\n):\n    \"\"\"Convert a Keras model to dot format.\n\n    Arguments:\n      model: A Keras model instance.\n      show_shapes: whether to display shape information.\n      show_layer_names: whether to display layer names.\n      rankdir: `rankdir` argument passed to PyDot,\n          a string specifying the format of the plot:\n          'TB' creates a vertical plot;\n          'LR' creates a horizontal plot.\n      expand_nested: whether to expand nested models into clusters.\n      dpi: Dots per inch.\n      style: value 0,1.\n      color: whether to display color.\n      subgraph: whether to return a `pydot.Cluster` instance.\n\n    Returns:\n      A `pydot.Dot` instance representing the Keras model or\n      a `pydot.Cluster` instance representing nested model if\n      `subgraph=True`.\n\n    Raises:\n      ImportError: if graphviz or pydot are not available.\n    \"\"\"\n    from tensorflow.keras import Model as Network\n    from tensorflow.keras.layers import Wrapper\n    from tensorflow.python.keras.engine import sequential\n\n    if not check_pydot():\n        if \"IPython.core.magics.namespace\" in sys.modules:\n            # We don't raise an exception here in order to avoid crashing notebook\n            # tests where graphviz is not available.\n            print(\"Failed to import pydot. You must install pydot\" \" and graphviz for `pydotprint` to work.\")\n            return\n        else:\n            raise ImportError(\n                \"Failed to import pydot. You must install pydot\" \" and graphviz for `pydotprint` to work.\"\n            )\n\n    if subgraph:\n        dot = pydot.Cluster(style=\"dashed\", graph_name=model.name)\n        dot.set(\"label\", model.name)\n        dot.set(\"labeljust\", \"l\")\n    else:\n        dot = pydot.Dot()\n        dot.set(\"rankdir\", rankdir)\n        dot.set(\"concentrate\", True)\n        dot.set(\"dpi\", dpi)\n        dot.set_node_defaults(shape=\"record\")\n\n    sub_n_first_node = {}\n    sub_n_last_node = {}\n    sub_w_first_node = {}\n    sub_w_last_node = {}\n\n    if not model._is_graph_network:\n        node = pydot.Node(str(id(model)), label=model.name)\n        dot.add_node(node)\n        return dot\n    elif isinstance(model, sequential.Sequential):\n        if not model.built:\n            model.build()\n    layers = model.layers\n    len(layers)\n\n    # Create graph nodes.\n    for i, layer in enumerate(layers):\n        layer_id = str(id(layer))\n\n        # Append a wrapped layer's label to node's label, if it exists.\n        layer_name = layer.name\n        class_name = layer.__class__.__name__\n        class_name_lower = class_name.lower()\n        config = 0\n        try:\n            config = layer.get_config()\n        except Exception:\n            pass\n\n        if isinstance(layer, Wrapper):\n            if expand_nested and isinstance(layer.layer, Network):\n                submodel_wrapper = model_to_dot(\n                    layer.layer, show_shapes, show_layer_names, rankdir, expand_nested, subgraph=True\n                )\n                # sub_w : submodel_wrapper\n                sub_w_nodes = submodel_wrapper.get_nodes()\n                sub_w_first_node[layer.layer.name] = sub_w_nodes[0]\n                sub_w_last_node[layer.layer.name] = sub_w_nodes[-1]\n                dot.add_subgraph(submodel_wrapper)\n            else:\n                layer_name = \"{}({})\".format(layer_name, layer.layer.name)\n                child_class_name = layer.layer.__class__.__name__\n                class_name = \"{}({})\".format(class_name, child_class_name)\n\n        if expand_nested and isinstance(layer, Network):\n            submodel_not_wrapper = model_to_dot(\n                layer, show_shapes, show_layer_names, rankdir, expand_nested, subgraph=True\n            )\n            # sub_n : submodel_not_wrapper\n            sub_n_nodes = submodel_not_wrapper.get_nodes()\n            sub_n_first_node[layer.name] = sub_n_nodes[0]\n            sub_n_last_node[layer.name] = sub_n_nodes[-1]\n            dot.add_subgraph(submodel_not_wrapper)\n\n        # Create node's label.\n\n        if show_layer_names:\n            label = \"{}: {}\".format(layer_name, class_name)\n            inputs = re.compile(\"input\")\n            if inputs.findall(class_name_lower):\n                pass\n            else:\n                if config != 0:\n                    conv = re.compile(\"conv\")\n                    if conv.findall(class_name_lower):\n                        label = \"{}:{},{}|kernel:{}  strides:{}\".format(\n                            layer_name, class_name, config[\"padding\"], config[\"kernel_size\"], config[\"strides\"]\n                        )\n                    pool = re.compile(\"pool\")\n                    if pool.findall(class_name_lower) and class_name_lower[:6] != \"global\":\n                        label = \"{}:{},{}|kernel:{}  strides:{}\".format(\n                            layer_name, class_name, config[\"padding\"], config[\"pool_size\"], config[\"strides\"]\n                        )\n                    activation = re.compile(\"activation\")\n                    if activation.findall(class_name_lower):\n                        label = \"{}:{}|{}\".format(layer_name, class_name, config[\"activation\"])\n                    dropout = re.compile(\"dropout\")\n                    if dropout.findall(class_name_lower):\n                        label = \"{}:{}|{}\".format(layer_name, class_name, config[\"rate\"])\n                    dense = re.compile(\"dense\")\n                    if dense.findall(class_name_lower):\n                        label = \"{}:{}|{}\".format(layer_name, class_name, config[\"activation\"])\n\n        else:\n            label = \"{}\".format(class_name)\n            inputs = re.compile(\"input\")\n            if inputs.findall(class_name_lower):\n                pass\n            else:\n                if config != 0:\n                    conv = re.compile(\"conv\")\n                    if conv.findall(class_name_lower):\n                        label = \"{},{}|kernel:{}  strides:{}\".format(\n                            class_name, config[\"padding\"], config[\"kernel_size\"], config[\"strides\"]\n                        )\n                    pool = re.compile(\"pool\")\n                    if pool.findall(class_name_lower) and class_name_lower[:6] != \"global\":\n                        label = \"{},{}|kernel:{}  strides:{}\".format(\n                            class_name, config[\"padding\"], config[\"pool_size\"], config[\"strides\"]\n                        )\n                    activation = re.compile(\"activation\")\n                    if activation.findall(class_name_lower):\n                        label = \"{}|{}\".format(class_name, config[\"activation\"])\n                    dropout = re.compile(\"dropout\")\n                    if dropout.findall(class_name_lower):\n                        label = \"{}|{}\".format(class_name, config[\"rate\"])\n                    dense = re.compile(\"dense\")\n                    if dense.findall(class_name_lower):\n                        label = \"{}|{}\".format(class_name, config[\"activation\"])\n\n        # Rebuild the label as a table including input/output shapes.\n        if show_shapes:\n\n            def format_shape(shape):\n                return str(shape).replace(str(None), \"None\")\n\n            try:\n                outputlabels = format_shape(layer.output_shape)\n            except AttributeError:\n                outputlabels = \"?\"\n            if hasattr(layer, \"input_shape\"):\n                inputlabels = format_shape(layer.input_shape)\n            elif hasattr(layer, \"input_shapes\"):\n                inputlabels = \", \".join([format_shape(ishape) for ishape in layer.input_shapes])\n            else:\n                inputlabels = \"?\"\n\n            if style == 0:\n                inputs = re.compile(\"input\")\n                if inputs.findall(class_name_lower):\n                    label = \"{%s}|{input:}|{%s}\" % (label, inputlabels)\n                else:\n                    for i, node in enumerate(layer._inbound_nodes):\n                        for outbound_layer in nest.flatten(node.outbound_layer):\n                            if outbound_layer.outbound_nodes == []:\n                                label = \"{%s}|{output:}|{%s}\" % (label, outputlabels)\n                            else:\n                                label = \"{%s}\" % (label)\n            elif style == 1:\n                label = \"{%s}|{input:|output:}|{{%s}|{%s}}\" % (label, inputlabels, outputlabels)\n\n        if not expand_nested or not isinstance(layer, Network):\n            if color is True:\n                inputs = re.compile(\"input\")\n                conv = re.compile(\"conv\")\n                pool = re.compile(\"pool\")\n                normalization = re.compile(\"normalization\")\n                activation = re.compile(\"activation\")\n                dropout = re.compile(\"dropout\")\n                dense = re.compile(\"dense\")\n                padding = re.compile(\"padding\")\n                concatenate = re.compile(\"concatenate\")\n                rnn = re.compile(\"rnn\")\n                lstm = re.compile(\"lstm\")\n                gru = re.compile(\"gru\")\n                bidirectional = re.compile(\"bidirectional\")\n                if inputs.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"deeppink\", style=\"filled\")\n                elif conv.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"cyan\", style=\"filled\")\n                elif pool.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"chartreuse\", style=\"filled\")\n                elif normalization.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"dodgerblue1\", style=\"filled\")\n                elif activation.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"pink\", style=\"filled\")\n                elif dropout.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"darkorange\", style=\"filled\")\n                elif dense.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"darkorchid1\", style=\"filled\")\n                elif padding.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"beige\", style=\"filled\")\n                elif concatenate.findall(class_name_lower):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"tomato\", style=\"filled\")\n                elif (\n                    rnn.findall(class_name_lower)\n                    or lstm.findall(class_name_lower)\n                    or gru.findall(class_name_lower)\n                    or bidirectional.findall(class_name_lower)\n                ):\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"yellow1\", style=\"filled\")\n                else:\n                    node = pydot.Node(layer_id, label=label, fillcolor=\"gold\", style=\"filled\")\n            else:\n                node = pydot.Node(layer_id, label=label)\n            dot.add_node(node)\n\n    # Connect nodes with edges.\n    for j, layer in enumerate(layers):\n        # print(layer)\n        # print(layer.output_shape)\n        def format_shape(shape):\n            return \" \" + str(shape).replace(str(None), \"None\")\n\n        layer_id = str(id(layer))\n        for i, node in enumerate(layer._inbound_nodes):\n            node_key = layer.name + \"_ib-\" + str(i)\n            if node_key in model._network_nodes:\n                for inbound_layer in nest.flatten(node.inbound_layers):\n                    inbound_layer_id = str(id(inbound_layer))\n                    if not expand_nested:\n                        assert dot.get_node(inbound_layer_id)\n                        assert dot.get_node(layer_id)\n                        if style == 0:\n                            try:\n                                add_edge(dot, inbound_layer_id, layer_id, format_shape(inbound_layer.output_shape))\n                            except Exception:\n                                add_edge(dot, inbound_layer_id, layer_id, \"?\")\n                        elif style == 1:\n                            add_edge(dot, inbound_layer_id, layer_id)\n                    else:\n                        # if inbound_layer is not Model or wrapped Model\n                        if not isinstance(inbound_layer, Network) and not is_wrapped_model(inbound_layer):\n                            # if current layer is not Model or wrapped Model\n                            if not isinstance(layer, Network) and not is_wrapped_model(layer):\n                                assert dot.get_node(inbound_layer_id)\n                                assert dot.get_node(layer_id)\n                                if style == 0:\n                                    try:\n                                        add_edge(\n                                            dot, inbound_layer_id, layer_id, format_shape(inbound_layer.output_shape)\n                                        )\n                                    except Exception:\n                                        add_edge(dot, inbound_layer_id, layer_id, \"?\")\n                                elif style == 1:\n                                    add_edge(dot, inbound_layer_id, layer_id)\n                            # if current layer is Model\n                            elif isinstance(layer, Network):\n                                if style == 0:\n                                    add_edge(\n                                        dot,\n                                        inbound_layer_id,\n                                        sub_n_first_node[layer.name].get_name(),\n                                        format_shape(inbound_layer.output_shape),\n                                    )\n                                elif style == 1:\n                                    add_edge(dot, inbound_layer_id, sub_n_first_node[layer.name].get_name())\n                            # if current layer is wrapped Model\n                            elif is_wrapped_model(layer):\n                                if style == 0:\n                                    try:\n                                        add_edge(\n                                            dot, inbound_layer_id, layer_id, format_shape(inbound_layer.output_shape)\n                                        )\n                                    except Exception:\n                                        add_edge(dot, inbound_layer_id, layer_id, \"?\")\n                                    name = sub_w_first_node[layer.layer.name].get_name()\n                                    add_edge(dot, layer_id, name, format_shape(layer.output_shape))\n                                elif style == 1:\n                                    add_edge(dot, inbound_layer_id, layer_id)\n                                    name = sub_w_first_node[layer.layer.name].get_name()\n                                    add_edge(dot, layer_id, name)\n                        # if inbound_layer is Model\n                        elif isinstance(inbound_layer, Network):\n                            name = sub_n_last_node[inbound_layer.name].get_name()\n                            if isinstance(layer, Network):\n                                output_name = sub_n_first_node[layer.name].get_name()\n                                if style == 0:\n                                    try:\n                                        add_edge(dot, name, output_name, format_shape(layer.output_shape))\n                                    except Exception:\n                                        add_edge(dot, name, output_name, \"?\")\n                                elif style == 1:\n                                    add_edge(dot, name, output_name)\n                            else:\n                                if style == 0:\n                                    try:\n                                        add_edge(dot, name, layer_id, format_shape(layer.output_shape))\n                                    except Exception:\n                                        add_edge(dot, name, layer_id, \"?\")\n                                elif style == 1:\n                                    add_edge(dot, name, layer_id)\n                        # if inbound_layer is wrapped Model\n                        elif is_wrapped_model(inbound_layer):\n                            inbound_layer_name = inbound_layer.layer.name\n                            if style == 0:\n                                try:\n                                    add_edge(\n                                        dot,\n                                        sub_w_last_node[inbound_layer_name].get_name(),\n                                        layer_id,\n                                        format_shape(inbound_layer.output_shape),\n                                    )\n                                except Exception:\n                                    add_edge(dot, sub_w_last_node[inbound_layer_name].get_name(), layer_id, \"?\")\n                            elif style == 1:\n                                add_edge(dot, sub_w_last_node[inbound_layer_name].get_name(), layer_id)\n\n    return dot\n</code></pre>"},{"location":"reference/runn/plot_model/#runn.plot_model.plot_model","title":"<code>plot_model(model, to_file='./model.png', show_shapes=True, show_layer_names=False, rankdir='TB', expand_nested=False, style=0, color=True, dpi=96)</code>","text":"<p>Converts a Keras model to dot format and save to a file.</p> PARAMETER  DESCRIPTION <code>model</code> <p>A Keras model instance</p> <p> </p> <code>to_file</code> <p>File name of the plot image.</p> <p> DEFAULT: <code>'./model.png'</code> </p> <code>show_shapes</code> <p>whether to display shape information.</p> <p> DEFAULT: <code>True</code> </p> <code>show_layer_names</code> <p>whether to display layer names.</p> <p> DEFAULT: <code>False</code> </p> <code>rankdir</code> <p><code>rankdir</code> argument passed to PyDot,   a string specifying the format of the plot:   'TB' creates a vertical plot;   'LR' creates a horizontal plot.</p> <p> DEFAULT: <code>'TB'</code> </p> <code>expand_nested</code> <p>Whether to expand nested models into clusters.</p> <p> DEFAULT: <code>False</code> </p> <code>style</code> <p>value 0,1.</p> <p> DEFAULT: <code>0</code> </p> <code>color</code> <p>whether to display color.</p> <p> DEFAULT: <code>True</code> </p> <code>dpi</code> <p>Dots per inch.</p> <p> DEFAULT: <code>96</code> </p> RETURNS DESCRIPTION <p>A Jupyter notebook Image object if Jupyter is installed.</p> <p>This enables in-line display of the model plots in notebooks.</p> Source code in <code>runn/plot_model.py</code> <pre><code>def plot_model(\n    model,\n    to_file=\"./model.png\",\n    show_shapes=True,\n    show_layer_names=False,\n    rankdir=\"TB\",\n    expand_nested=False,\n    style=0,\n    color=True,\n    dpi=96,\n):\n    \"\"\"Converts a Keras model to dot format and save to a file.\n\n    Arguments:\n      model: A Keras model instance\n      to_file: File name of the plot image.\n      show_shapes: whether to display shape information.\n      show_layer_names: whether to display layer names.\n      rankdir: `rankdir` argument passed to PyDot,\n          a string specifying the format of the plot:\n          'TB' creates a vertical plot;\n          'LR' creates a horizontal plot.\n      expand_nested: Whether to expand nested models into clusters.\n      style: value 0,1.\n      color: whether to display color.\n      dpi: Dots per inch.\n\n    Returns:\n      A Jupyter notebook Image object if Jupyter is installed.\n      This enables in-line display of the model plots in notebooks.\n    \"\"\"\n    assert style == 0 or style == 1\n    dot = model_to_dot(\n        model,\n        show_shapes=show_shapes,\n        show_layer_names=show_layer_names,\n        rankdir=rankdir,\n        expand_nested=expand_nested,\n        style=style,\n        color=color,\n        dpi=dpi,\n    )\n    if dot is None:\n        return\n    _, extension = os.path.splitext(to_file)\n    if not extension:\n        extension = \"png\"\n    else:\n        extension = extension[1:]\n    # Save image to disk.\n    dot.write(to_file, format=extension)\n    # Return the image as a Jupyter Image object, to be displayed in-line.\n    # Note that we cannot easily detect whether the code is running in a\n    # notebook, and thus we always return the Image if Jupyter is available.\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n</code></pre>"}]}